{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd237a6",
   "metadata": {},
   "source": [
    "# Prefix Fine-tuning\n",
    "This notebook demonstrates an example of [prefix tuning](https://arxiv.org/pdf/2101.00190.pdf), where learned embeddings allow for a comparatively low-memory method to fine-tune behavior in a large language model via encoding behavior/information in a small number of embedding tokens. This notebook demonstrates an example of fine-tuning GPT-2 XL to translate from French to English and vice versa. The way this works is essentially as follows: we learn two prefixes, `[start_english]` and `[start_french]`. When we want to translate a French sentence to English, we would input the following prompt to the language model:\n",
    "\n",
    "`[start_french]Bonjour! Je m'appelle Sully.[start_english]`.\n",
    "\n",
    "We can also do the same task via few-shot prompting of the language model. At the end of this notebook, we compare (via [BLEU-score](https://en.wikipedia.org/wiki/BLEU)) the performance of prefix-tuning with few-shot translation and show a marked improvement. Ideally, we would do a comparison of fine-tuning the whole model on the dataset vs. prefix-tuning, but I don't have enough GPU memory on my machine to do that test. Anyway, we see a ~2x improvement in BLEU score via prefix-tuning versus 3-shot prompting!\n",
    "\n",
    "## Requirements\n",
    "- [Dataset](https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset) (we won't use all of the dataset, only ~200k examples of the dataset for training and evaluation. If you don't have enough memory to load the whole dataset, just load a subset instead!)\n",
    "- Pytorch (for machine learning)\n",
    "- NTLK (for BLEU score)\n",
    "- Numpy (for math)\n",
    "- TQDM (for progress bars)\n",
    "- HuggingFace `transformers` (for GPT-2 XL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e338288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #ML\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable #these will help us define the prefixes we will optimize\n",
    "import numpy as np #math\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel #GPT-2 XL and its tokenizer\n",
    "from tqdm import tqdm #progress bar\n",
    "import csv #reading the CSV\n",
    "from nltk.translate.bleu_score import sentence_bleu #BLEU score computation\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe79aac",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "For ease of use/clarity, we'll load the English/French sentences pairs into two separate arrays that will correspond by index. This is not the best code practice, as a deletion or addition in either array will cause misalignment of the entire dataset, but for the purpose of clarity and for this notebook we'll let it slide!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b86468",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = [] #array containing the English sentences as strings\n",
    "french_sentences = [] #array containing the French sentences as strings\n",
    "\n",
    "#Load the dataset\n",
    "with open('en-fr.csv', newline='\\n', encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        english_sentences.append(row[0])\n",
    "        french_sentences.append(row[1])\n",
    "        \n",
    "        #restrict data loading to just 200k examples\n",
    "        if len(english_sentences) >= 200000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0be3c8",
   "metadata": {},
   "source": [
    "## Load model, tokenizer, and embeddings\n",
    "In this section, we load the GPT-2 XL model and its corresponding tokenizer. Next, we look at the word embedding matrix shape to get the vocab length and embedding size. Lastly, we extract the embedding matrix from the model file, then convert it to a list of embeddings that we can index easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c197d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl') #tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device) #model\n",
    "\n",
    "vocab_len, embed_size = tuple(model.state_dict()['transformer.wte.weight'].shape)\n",
    "embedding_matrix = model.state_dict()['transformer.wte.weight'].clone().cpu() #actual embedding matrix\n",
    "\n",
    "#we create a custom tokenizer function here, because we want to return an array of embeddings rather than \n",
    "#an array of indices\n",
    "def tokenize(string):\n",
    "    \"\"\"\n",
    "    Tokenizes a string and converts it to a tensor of embeddings.\n",
    "    \n",
    "    Args:\n",
    "    string (str): The input string to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "    A tensor of embeddings for the input string.\n",
    "    \"\"\"\n",
    "    # Tokenize the string using the tokenizer\n",
    "    x = torch.tensor(tokenizer(string)['input_ids']).view(1, -1)\n",
    "    \n",
    "    # Compute the prompt length and embeddings\n",
    "    prompt_len = x.shape[-1]\n",
    "    prompt_embeddings = F.one_hot(x, num_classes=vocab_len).float() @ embedding_matrix\n",
    "    \n",
    "    # Return the prompt embeddings on the device\n",
    "    return prompt_embeddings.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908844e8",
   "metadata": {},
   "source": [
    "## Training\n",
    "Next, the fun part — learning the prefixes. First, we need to turn off the storage of gradients in the model parameters. This will save a ton of memory, as we won't have to store ~1.5B parameters worth of gradients.\n",
    "\n",
    "Next, we define the prefixes as PyTorch autograd variables to be optimized. We also define a function that will create our example prompts along with the corresponding ground truth target.\n",
    "\n",
    "Lastly, we define an optimizer ([Adam](https://arxiv.org/pdf/1412.6980.pdf)), as well as a linear learning rate decay, and begin training!\n",
    "\n",
    "As a quick note, we use gradient accumulation in place of batch training because we don't have enough memory to do normal batches. There, technically, a mathematical inconsistency in how we've implemented the accumulation, but it's not super important. Since we train on many sentences of different length, but accumulate each gradient weighted equally, the resultant averaged gradient will be different than a true padded batch gradient. This isn't super important though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8eed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set each parameter of the model to not store gradients\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc38216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define learnable prompts\n",
    "\n",
    "tuning_length = 16 #number of tokens for each learned prefix\n",
    "\n",
    "#Define the prefixes, initializing them to a random unit norm\n",
    "start_english_prompt = Variable(torch.randn(embedding_matrix.shape[-1]*tuning_length, \n",
    "                                            device=device).view(1, tuning_length, -1), requires_grad=True)\n",
    "start_french_prompt = Variable(torch.randn(embedding_matrix.shape[-1]*tuning_length, \n",
    "                                           device=device).view(1, tuning_length, -1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49518edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(lang1_str, lang2_str, start_lang1_prompt, start_lang2_prompt):\n",
    "    \"\"\"\n",
    "    Concatenates the prefixes and the two input language strings and returns the resulting tensor and the\n",
    "    target ground-truth tensor for the second language string.\n",
    "    \n",
    "    Args:\n",
    "    lang1_str (str): The first language string.\n",
    "    lang2_str (str): The second language string.\n",
    "    start_lang1_prompt (torch.Tensor): The tensor representing the prefix for the first language string.\n",
    "    start_lang2_prompt (torch.Tensor): The tensor representing the prefix for the second language string.\n",
    "    \n",
    "    Returns:\n",
    "    A tuple containing the concatenated tensor of the prefixes and the two input language strings, and \n",
    "    the ground truth tensor for training.\n",
    "    \"\"\"\n",
    "    lang1 = tokenize(lang1_str)\n",
    "    lang2 = tokenize(lang2_str)\n",
    "        \n",
    "    out = torch.concat((start_lang1_prompt, lang1, start_lang2_prompt, lang2), dim=1)\n",
    "    \n",
    "    #we had an EOS token to the target so that we can know when the translation is done generating\n",
    "    return out, torch.tensor(tokenizer(lang2_str + \"<|endoftext|>\")['input_ids']).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 2048 #number of iterations to run training for\n",
    "\n",
    "optimizer = torch.optim.Adam([start_english_prompt, start_french_prompt], lr=0.1) #our optimizer\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, #our scheduler\n",
    "                                              end_factor=0.001, total_iters=iters)\n",
    "\n",
    "loss_hist = [] #keep track of our loss history\n",
    "\n",
    "accumulation_steps = 64 #we use gradient accumulation because we don't have enough memory to do batches\n",
    "\n",
    "pbar = tqdm(range(0, iters)) #progress bar\n",
    "\n",
    "for step in pbar:\n",
    "    optimizer.zero_grad() #zero gradients\n",
    "    \n",
    "    steps = 0 #keep track of how many steps we've accumulated\n",
    "    total_loss = 0 #keep track okf the total loss for our records\n",
    "    while steps < accumulation_steps: #accumulation loop\n",
    "        index = np.random.randint(0, int(len(english_sentences)*0.8)) #pick a random sentence\n",
    "        \n",
    "        order = np.random.randint(0, 2) #pick a random order (i.e. en -> fr or fr-> en)\n",
    "        \n",
    "        #create the input and target\n",
    "        if order == 0:\n",
    "            embeddings, target = create_example(french_sentences[index], english_sentences[index], \n",
    "                                                start_french_prompt, start_english_prompt)\n",
    "        else:\n",
    "            embeddings, target = create_example(english_sentences[index], french_sentences[index],\n",
    "                                                start_english_prompt, start_french_prompt)\n",
    "        \n",
    "        #so this is a bit annoying, but I don't have enough GPU memory to input more than a 384 length prompt \n",
    "        #during train time, so we restrict our training to examples of max length 384. This isn't a huge issue, \n",
    "        #but there may be diminished performance at test time if we are translating large paragraphs.\n",
    "        if embeddings.shape[1] < 384: #memory constraints\n",
    "            out = model(inputs_embeds=embeddings.to(device))['logits'] #compute logits\n",
    "            loss = F.cross_entropy(out[:, -target.shape[-1]:].view(-1, out.size(-1)), #compute CE loss\n",
    "                                   target.view(-1).to(device)) / accumulation_steps\n",
    "            loss.backward() #backwards pass\n",
    "            \n",
    "            total_loss += loss.item() / accumulation_steps #keep track of loss\n",
    "\n",
    "            steps += 1 #keep track of a successful gradient accumulation step\n",
    "    \n",
    "    loss_hist.append(total_loss) #keep track of loss history\n",
    "    pbar.set_description(f\"Loss: {total_loss}\") #set the progress bar description\n",
    "    \n",
    "    optimizer.step() #update parameters\n",
    "    scheduler.step() #update learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f1022",
   "metadata": {},
   "source": [
    "## Test the model performance\n",
    "First, we define our few-shot prompts. These are hand-crafted via examples from our dataset. Next, we define some helper functions to sample from our model via these prefixes. Lastly, we compute the BLEU score for the few-shot vs. prefix tuning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a46551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These are our few-shot prompts for the translation task\n",
    "prompt_3_shot_en_fr=\"\"\"\n",
    "en: Considerable attention is also given to ensuring access for aboriginal communities, particularly in the Labrador region.\n",
    "fr: La proposition ne traite pas de la langue de prestation des services en particulier.\n",
    "\n",
    "en: In 2005 Canada imported $332.4 million of crude oil from Angola.\n",
    "fr: En 2005, le Canada a importé d’Angola pour 332,4 millions de dollars de pétrole brut.\n",
    "\n",
    "en: Independent (FIT) leisure travel is expected to decrease 3 per cent, but this will be partially off-set by a 2 per cent increase in group leisure travel.\n",
    "fr: Australie De l’avis des participants à l’APM, le tourisme d’agrément en provenance de l’Australie ne devrait diminuer dans l’ensemble que de 1 p.\n",
    "\n",
    "en:\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_3_shot_fr_en=\"\"\"\n",
    "fr: La proposition ne traite pas de la langue de prestation des services en particulier.\n",
    "en: Considerable attention is also given to ensuring access for aboriginal communities, particularly in the Labrador region.\n",
    "\n",
    "fr: En 2005, le Canada a importé d’Angola pour 332,4 millions de dollars de pétrole brut.\n",
    "en: In 2005 Canada imported $332.4 million of crude oil from Angola.\n",
    "\n",
    "fr: Australie De l’avis des participants à l’APM, le tourisme d’agrément en provenance de l’Australie ne devrait diminuer dans l’ensemble que de 1 p.\n",
    "en: Independent (FIT) leisure travel is expected to decrease 3 per cent, but this will be partially off-set by a 2 per cent increase in group leisure travel.\n",
    "\n",
    "fr:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23685c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_argmax(start_lang1_prompt, input_str, start_lang2_prompt, verbose=True):\n",
    "    \"\"\"\n",
    "    Translates an input language string to the target language using the argmax method.\n",
    "    \n",
    "    Args:\n",
    "    start_lang1_prompt (torch.Tensor): The tensor representing the prefix for the input language.\n",
    "    input_str (str): The input language string to be translated.\n",
    "    start_lang2_prompt (torch.Tensor): The tensor representing the prefix for the target language.\n",
    "    verbose (bool): If True, the function prints the predicted tokens as they are generated. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    A list of token IDs representing the translated target language string.\n",
    "    \"\"\"\n",
    "    lang1 = tokenize(input_str) #tokenize input\n",
    "    out = torch.concat((start_lang1_prompt, lang1, start_lang2_prompt), dim=1) #create input prompt\n",
    "    input_len = out.shape[1] #get the input length\n",
    "    \n",
    "    if input_len > 512: #memory constraints :(\n",
    "        return 0\n",
    "    \n",
    "    with torch.no_grad(): #use no_grad to save memory\n",
    "        out_sequence = [] #will be our output sequence\n",
    "        \n",
    "        #generate our first output token\n",
    "        out = model(inputs_embeds=out.to(device))\n",
    "        out_sequence.append(out['logits'].argmax(dim=-1).flatten()[-1].item())\n",
    "        \n",
    "        if verbose:\n",
    "            print(tokenizer.decode(out_sequence[-1]), end='')\n",
    "        \n",
    "        #we terminate generation either when we see the EOS token, *or* when the length of the generation \n",
    "        #is greater than the context length, *or* when the length of the generation is ~2x the length of \n",
    "        #the input sequence. This last termination clause is just to not waste our time when the model \n",
    "        #gets stuck on some repetitive or non-sense generation.\n",
    "        while out_sequence[-1] != 50256 and len(out_sequence) + input_len < min(1024, input_len*2):\n",
    "            out = model(inputs_embeds=embedding_matrix[out_sequence[-1]].view(1, -1).to(device), \n",
    "                        past_key_values=out['past_key_values']) #use KV recycling to save compute!\n",
    "            out_sequence.append(out['logits'].argmax(dim=-1)[-1].item())\n",
    "            if verbose:\n",
    "                if out_sequence[-1] != 50256:\n",
    "                    print(tokenizer.decode(out_sequence[-1]), end='')\n",
    "    \n",
    "    return out_sequence[:-1] #return all but the EOS token\n",
    "\n",
    "def en_fr_3_shot_argmax(input_str, verbose=True):\n",
    "    \"\"\"\n",
    "    Translates a given English input string to French using a 3-shot prompt.\n",
    "    \n",
    "    Args:\n",
    "    input_str (str): The input English string to be translated.\n",
    "    verbose (bool, optional): If True, the function will print the translation as it is being generated. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    out_sequence (list): A list of tokens representing the generated French translation.\n",
    "    \"\"\"\n",
    "    out = tokenize(prompt_3_shot_en_fr + \" \" + input_str.strip() + \"\\nfr:\") #prepare input prompt\n",
    "    input_len = out.shape[1] #get the length of the input\n",
    "    \n",
    "    if input_len > 512: #memory constraints :(\n",
    "        return 0\n",
    "    \n",
    "    with torch.no_grad(): #save memory with no_grad\n",
    "        out_sequence = []\n",
    "        out = model(inputs_embeds=out.to(device)) #sample first token\n",
    "        out_sequence.append(out['logits'].argmax(dim=-1).flatten()[-1].item())\n",
    "        \n",
    "        if verbose:\n",
    "            print(tokenizer.decode(out_sequence[-1]), end='')\n",
    "        \n",
    "        #we terminate generation either when we see a newline token, *or* when the length of the generation \n",
    "        #is greater than the context length, *or* when the length of the generation is ~2x the length of \n",
    "        #the input sequence. This last termination clause is just to not waste our time when the model \n",
    "        #gets stuck on some repetitive or non-sense generation.\n",
    "        while out_sequence[-1] != 198 and len(out_sequence) + input_len < min(1024, input_len*2):\n",
    "            out = model(inputs_embeds=embedding_matrix[out_sequence[-1]].view(1, -1).to(device), \n",
    "                        past_key_values=out['past_key_values'])\n",
    "            out_sequence.append(out['logits'].argmax(dim=-1)[-1].item())\n",
    "            if verbose:\n",
    "                if out_sequence[-1] != 198:\n",
    "                    print(tokenizer.decode(out_sequence[-1]), end='')\n",
    "    \n",
    "    return out_sequence[:-1]\n",
    "\n",
    "def fr_en_3_shot_argmax(input_str, verbose=True):\n",
    "    \"\"\"\n",
    "    Translates a given French input string to English using a 3-shot prompt.\n",
    "    \n",
    "    Args:\n",
    "    input_str (str): The input French string to be translated.\n",
    "    verbose (bool, optional): If True, the function will print the translation as it is being generated. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    out_sequence (list): A list of tokens representing the generated English translation.\n",
    "    \"\"\"\n",
    "    out = tokenize(prompt_3_shot_fr_en + \" \" + input_str.strip() + \"\\nen:\") #prepare input prompt \n",
    "    input_len = out.shape[1] #get the length of the input\n",
    "    \n",
    "    if input_len > 512: #memory constraints :(\n",
    "        return 0\n",
    "    \n",
    "    with torch.no_grad(): #save memory with no_grad\n",
    "        out_sequence = []\n",
    "        out = model(inputs_embeds=out.to(device)) #sample first token\n",
    "        out_sequence.append(out['logits'].argmax(dim=-1).flatten()[-1].item())\n",
    "        \n",
    "        if verbose:\n",
    "            print(tokenizer.decode(out_sequence[-1]), end='')\n",
    "        \n",
    "        #we terminate generation either when we see a newline token, *or* when the length of the generation \n",
    "        #is greater than the context length, *or* when the length of the generation is ~2x the length of \n",
    "        #the input sequence. This last termination clause is just to not waste our time when the model \n",
    "        #gets stuck on some repetitive or non-sense generation.\n",
    "        while out_sequence[-1] != 198 and len(out_sequence) + input_len < min(1024, input_len*2):\n",
    "            out = model(inputs_embeds=embedding_matrix[out_sequence[-1]].view(1, -1).to(device), \n",
    "                        past_key_values=out['past_key_values'])\n",
    "            out_sequence.append(out['logits'].argmax(dim=-1)[-1].item())\n",
    "            if verbose:\n",
    "                if out_sequence[-1] != 198:\n",
    "                    print(tokenizer.decode(out_sequence[-1]), end='')\n",
    "    \n",
    "    return out_sequence[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare BLEU scores of each approach\n",
    "\n",
    "k_shot_BLEU = [] #will store our BLEU scores for prompting\n",
    "k_shot_pairs = [] #will store our ground-truth and predicted strings\n",
    "\n",
    "tuning_BLEU = [] #will store our BLEU scores for prefix-tuning\n",
    "tuning_pairs = [] #will store our ground-truth and predicted strings\n",
    "\n",
    "\n",
    "pbar = tqdm(range(int(len(english_sentences)*0.8), len(english_sentences))) #progress bar\n",
    "for i in pbar:\n",
    "    #get the test sentence\n",
    "    en = english_sentences[i]\n",
    "    fr = french_sentences[i]\n",
    "    \n",
    "    #translate in each direction via the 3-shot prompt\n",
    "    k_shot_fr_out = tokenizer.decode(en_fr_3_shot_argmax(en, verbose=False))\n",
    "    k_shot_en_out = tokenizer.decode(fr_en_3_shot_argmax(fr, verbose=False))\n",
    "    \n",
    "    if k_shot_fr_out == 0 or k_shot_en_out == 0: #if errors in either translation, skip it\n",
    "        pass\n",
    "    \n",
    "    #translate in each direction via prefix-tuning\n",
    "    tuning_fr_out = tokenizer.decode(translate_argmax(start_english_prompt.to(device), en, \n",
    "                                                      start_french_prompt.to(device), verbose=False))\n",
    "    tuning_en_out = tokenizer.decode(translate_argmax(start_french_prompt.to(device), fr, \n",
    "                                                      start_english_prompt.to(device), verbose=False))\n",
    "    if tuning_fr_out == 0 or tuning_en_out == 0: #if errors in either translation, skip it\n",
    "        pass\n",
    "    \n",
    "    #compute BLEU scores\n",
    "    k_shot_BLEU.append(sentence_bleu([en.split()], k_shot_en_out.split()))\n",
    "    k_shot_BLEU.append(sentence_bleu([fr.split()], k_shot_fr_out.split()))\n",
    "    \n",
    "    tuning_BLEU.append(sentence_bleu([en.split()], tuning_en_out.split()))\n",
    "    tuning_BLEU.append(sentence_bleu([fr.split()], tuning_fr_out.split()))\n",
    "    \n",
    "    #append prediction pairs\n",
    "    tuning_pairs.append((en, tuning_en_out))\n",
    "    tuning_pairs.append((fr, tuning_fr_out))\n",
    "    \n",
    "    k_shot_pairs.append((en, k_shot_en_out))\n",
    "    k_shot_pairs.append((fr, k_shot_fr_out))\n",
    "    \n",
    "    #update progress bar\n",
    "    pbar.set_description(f\"3-shot BLEU: {np.mean(k_shot_BLEU):.5f}, tuning BLEU: {np.mean(tuning_BLEU):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display results!\n",
    "\n",
    "print(np.mean(tuning_BLEU), np.mean(k_shot_BLEU))\n",
    "print(np.std(tuning_BLEU), np.std(k_shot_BLEU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed70d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fca19a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
